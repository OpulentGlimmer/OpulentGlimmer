### Hi there 👋

![Anurag's GitHub stats](https://github-readme-stats.vercel.app/api?username=datastation7&theme=github_dark&show_icons=true)

![Top Langs](https://github-readme-stats.vercel.app/api/top-langs/?username=datastation7&layout=compact&theme=github_dark)

✍🏻 Recent blog posts

-  TIL
   -  Today I Learned의 약자이다.
   -  그 날 공부한 것들을 정리한 곳이다.
  
- Project
  - First project
    - SNIA는 SAN(Storage Area Network) 표준과 관련하여 70개 이상의 관련 업체가 구성한 국제 비영리 산업단체인데 여기서 데이터를 출처하였다.
    - Block I/O는 I/O작업이 진행되는 동안 유저 프로세스가 자신의 작업을 중단한채, I/O가 끝날때까지 대기하는 방식이다.
    - 이 방식을 도입해서 하드디스크가 과부하되는 것을 막고, 캐시 메모리가 필요성이 있다고 결론이 나왔다.
  
  -  Second project
     - 타이타닉 데이터를 활용해서 머신러닝 모델 중에서 GradientBoosting Classifier의 정확도가 가장 높게 산출되었다.
     - Grid Search 로 구한 가장 좋은 Hyper Parameter를 적용했을 때 모델 정확도가 0.85에서 0.82로 감소하였다.
     - Stacking 으로 구한 두 정확도 중 Train정확도보다 Test정확도가 낮았다.
     - 그 이유는 Cross Validation시 한 fold당 표본개수가 부족하기 때문이다.

  - Third project
    - 캐글 데이터를 이용해 당뇨병을 예측하는 프로젝트이다.
    - 고혈압 또는 뇌졸증이 있거나 계단을 오를 때 불편함을 겪는 경우 당뇨병 발병률이 더 높은 양상을 보였다.
    - Age features가 0부터 12까지 있고, 삭제를 했을 때는 0.90이 나오는데 삭제를 안 했을 경우에는 0.87로 나와서 Age features를 삭제하였다.
    - 머신러닝 중 성능이 가장 높은 모델은 Accuracy:0.7570, Recall:0.7987, PrecL0.7372, F1:0.7667지표가 가장 좋게 나왔으므로 Pycaret모델이 가장 좋은 모델이 판단되었다.
    - 딥러닝 중 성능이 가장 높은 모델은 Loss:0.5025, Accuracy:0.7533, Recall:0.8008, Precision:0.7268지표가 가장 좋게 나와서 Pipline적용한 모델이 가장 좋은 모델이라고 판단되었다.
    - 데이터가 대부분 이산형으로 정제되어 있어서 EDA 제약이 많아서 처음부터 전처리가 안 되어있는 데이터보다 모델 정확도가 떨어졌다.
   
   - Four Project
     - 이 프로젝트의 목표는 S제조사 반도체 라인 엔지니어의 데이터 기반 의사결정을 돕는 시스템을 구축하는 것이다.
     - 산업규모가 큰 만큼 팹에서는 매일 8PB의 아주 많은 데이터가 생성된다.
     - 이를 의사결정에 활용하기 위해서는 데이터를 안전하게 추출하고, 처리하고, 저장하는 ETL시스템이 필요하다.
     - 이러한 시스템을 구축하고 데이터 엔지니어의 현업을 직간접적으로 경험하기 위해서 이번 프로젝트를 진행하였다.
     - 데이터 엔지니어의 업무는 최종데이터 사용자, 즉 공장 엔지니어나 데이터 분석가의 요구사항을 분석하는 것에서부터 시작한다.
     - 데이터의 규모는 얼마나 되는지, 데이터를 어떤 형식으로 만들어야 하는지 등을 분석한다.
     - 과정은 크게 수집, 저장, 흐름관리, 스트리밍 분석, 제출용 데이터 제작으로 나누어진다.
     - 이 다섯가지 과정 중 저장, 흐름관리, 제출용 데이터 제작의 세가지 업무를 경험하였다.
     - 팹에서 발생한 데이터를 최종 사용자에게 전달하는 시나리오를 가정해 데이터 흐름을 구축하였다.
     - 팹에서 발생하는 말단 데이터는 팹 내부의 minIO 스토리지에 저장되고 나이파이를 사용해 이 데이터를 팹 외부 S3 스토리지에 옮겼다.
     - 이후 데이터브릭스 클라우드 환경에서 파이스파크 라이브러리를 사용해 데이터 저장과 전처리를
       SQL을 활용해 시각화와 대시보드 제작을 진행하였다.
     - 나이파이는 대량의 데이터를 플로우 프로세스로 구축, 유지, 교환하는 시스템이다.
     - gui를 통해 데이터 흐름을 눈으로 직접 보고 ETL프로세스를 설계할 수 있다.
     - Nifi를 사용해 앞서 설명한 데이터 엔지니어의 업무 중 데이터 저장과 흐름관리를 수행할 수 있다.
     - 나이파이의 가장 큰 장점은 gui를 통해 간편하게 데이터 흐름을 관리할 수 있다.
     - 프로세서를 실행시키면 gui를 통해 몇 개의 데이터가 흐르고 있는지, 오류는 어느 부분에서 발생했고 오류메세지는 무엇인지,
       어떤 프로세서가 실행되고 있는지 아주 쉽게 확인할 수 있다.
     - 스파크는 하나의 라이브러리로 SQL, 머신러닝, 시각화, 스트리밍 분석을 전부 할 수 있는 종합 빅데이터 엔진이다.
     - 파이선, 스칼라, 자바 등 여러 언어를 지원한다.
     - 스파크의 가장 큰 장점은 바로 빠른 속도인데요 메모리에서 데이터를 처리하여 기존 엔진에 비해 매우 빠른 속도를 가지고 있는
       강력한 도구라고 할 수 있다.
     - 스파크를 활용하여 빅데이터 엔지니어의 업무 중 흐름관리, 스트리밍 분석, 제출용 데이터 제작이 가능하다.
     - 데이터는 학습을 위해 임의로 만들어진 반도체 장비의 시간별 센서 데이터와 이를 설명하는 메타데이터로 이루어져 있다.
     - 메타데이터는 공정 장비 id 데이터 생성시간, 측정시행의 id, 그리고 a,b컬럼으로 이루어져 있다.
     - a,b 컬럼의 경우 2차원배열로 이루어져있으며 컬럼의 이름은 기업에서 실제로 사용하고 있는 컬럼의 이름이므로
       알파벳으로 나타낼 수밖에 없다는 점을 양해 부탁드립니다.




